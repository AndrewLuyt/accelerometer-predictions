---
title: 'Automated Exercise Assistant'
author: "Andrew Luyt"
date: "`r Sys.Date()`"
always_allow_html: yes
output: 
    github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include=FALSE)
library(knitr)
library(kableExtra)
library(tidyverse)
library(lubridate)
library(ModelMetrics) # do before caret so we don't mask e.g. confusionMatrix
library(caret)
library(randomForest)
library(gbm)
library(glmnet)
library(doMC) # parallel backend
registerDoMC(4)
```

![AI-generated image of a woman exercising with a smart device](ai_exercise.png)

# Purpose

To build a machine learning model that can accurately detect if a human is
performing an exercise using the incorrect form.
The model will only use measurements from 
accelerometers on the body & dumbbell while performing dumbbell bicep curls.

This project aims to improve upon the results of a 2013 experiment yielding
a model with 78.5% accuracy.

The model should be able to **detect and distinguish between** mistakes such as 
incorrect elbow position or only partially lifting the dumbbell. It should
also be, at least theoretically, implementable on a wearable device
such as a smart watch or FitBit.

If a person's errors in exercise form
could be detected automatically and in real-time, one 
possible application would be a wearable device
sending an audio notification to tell the user their form is suspect:
e.g. throwing elbows to the front or thrusting their hips forward. 

# Summary of Results

Finding that the data were not well-suited to the assumptions of Linear
Discriminant Analysis, **we were 
able to build random forest and boosted models with predictive accuracy of 99.3%.**

# The dataset

19623 observations over 160 variables, mostly numeric features extracted from
accelerometer data streams. 

The `classe` variable is our target. It holds five classes corresponding to correct & incorrect form while doing the exercise:

* **A**: correct form
* **B-E**: incorrect form (common mistakes)
    * B: throwing the elbows to the front
    * C: lifting the dumbbell only halfway
    * D: lowering the dumbbell only halfway
    * E: throwing the hips to the front
    
Variable selection will be an important part of this problem. 

These data come from a 2013 experiment, *Qualitative Activity Recognition of Weight Lifting Exercises* by Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.,
and was released under a Creative Commons licence. [See References for URLs]

# Benchmarks

The original researchers obtained 78.5% overall accuracy on this dataset, with class-specific accuracies between 74% and 86%.

![Figure by Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.](wle_confusion_matrix.png){width=50% height=50%}

# Cleaning and preprocessing

```{r}
# There are a few ways NAs show up in the data
df.train <- read_csv("pml-training.csv", na = c("NA", "#DIV/0!", ""))
df.test <- read_csv("pml-testing.csv", na = c("NA", "#DIV/0!", ""))
```

Using `skimr::skim()` we find that 100 variables have over 97% of their data missing.
We'll simply remove these variables. This greatly simplifies the problem of variable selection as we now only have 59 predictors available.

```{r}
df.train %>% skimr::skim() %>% 
    filter(complete_rate < 0.03) %>% 
    pull(skim_variable) %>% 
    length()
```

```{r}
vars.remove <- df.train %>% 
    skimr::skim() %>% 
    filter(complete_rate < 0.03) %>% 
    pull(skim_variable)
```

`classe` needs to be converted to factors.  
`cvtd_timestamp` is the time the user performed the exercise *during the original
experiment* and will have no bearing on *future* prediction so it will be
removed. `user_name` will be removed as well as we can't expect it to be
available in the future.

```{r}
vars.remove <- c(vars.remove, 'cvtd_timestamp')
```

`raw_timestamp_part_1` is *suspicious.*  In the plot below we can distinguish 
the blocks of time where each `classe` was performed by each subject. This appears
to be due simply to the way the original experiment was conducted: each subject
was asked to perform exercises with a given form one after another, and these times
simply reflect that sequence.  When an algorithm is applied to *future* data (for
example a person doing their workout tomorrow)
these time patterns will be completely different, rendering this feature useless. 
We will remove this variable along with `raw_timestamp_part_2`, 
and `new_window` for similar reasons.

```{r include=TRUE}
df.train %>% ggplot(aes(user_name, raw_timestamp_part_1, col=classe)) + geom_boxplot()
```

`num_window` has similar problems. Below, if for example `num_window` is
about 250, the `classe` will be **B**. This variable seems to be a sequence identifier
for blocks of information sequentially extracted from the accelerometer data 
stream during feature extraction. There's no reason to suspect this pattern 
would continue in any future data. This variable will also be removed.

```{r include=TRUE}
df.train %>% ggplot(aes(num_window, classe, col=classe)) + geom_point()
```


```{r}
vars.remove <- c(vars.remove, 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'num_window',
                 'user_name', 'new_window')
```

```{r}

clean_df <- function(df) {
    df <- df %>% 
        select(!...1) %>%  # row number, useless
        select(!all_of(vars.remove))
    if ("classe" %in% names(df)) {
        df <- mutate(df, classe = as_factor(classe))
    }
    df
}

df.train <- clean_df(df.train)
df.test <- clean_df(df.test)
```

## Check variables for normality

Some models, like LDA, assume normal distributions of variables - how good of 
an assumption would this be?  Using a Shapiro Test, we find that no variable is
drawn from a normal distribution. 

```{r}
set.seed(1712)
sample.df <- df.train %>% slice_sample(n = 3000) # shapiro needs < 5000 items
shapiro.results <- lapply(X = select(sample.df, where(is.numeric)), FUN = shapiro.test)
shapiro.results <- sapply(shapiro.results, function(i) i$p.value)
shapiro.results[shapiro.results > 1e-15]; rm(shapiro.results, sample.df)
```


## Check for near-zero-variance predictors

After removing variables full of NAs, `caret::nearZeroVar()` was used. All 
variables have sufficient variance to be useful, in theory.

```{r}
df.train %>% select(where(is.numeric)) %>% 
    nearZeroVar(saveMetrics = T, allowParallel = TRUE) %>% 
    filter(nzv==TRUE)
```

# Modeling

We'll explore a Linear Discriminant model first as a baseline and then
two ensemble models generally considered to give excellent
performance on classification problems: boosted trees and random forests.
 

```{r}
y <- df.train$classe
```

## Linear Discriminant Model

Earlier we found that
the variables are not normally distributed, so we can't expect good 
classification performance from LDA.

`classe` was fit to all variables and the resulting accuracy estimate via
a .632 bootstrap was 70%.  The original researchers obtained 78.5% using
a different model, so we will have to improve on this.

```{r include=TRUE}
set.seed(1712)
tc <- trainControl(method = "boot632", number = 10, allowParallel = TRUE)
fit.lda <- train(classe ~ ., method='lda', data=df.train, trControl=tc)
preds <- predict(fit.lda)
round(fit.lda$results[2:3], 3)
# postResample(preds, y)
# round(confusionMatrix(preds, y, )$overall, 3)
```

## Stochastic Gradient Boosting

Tree-based ensembles are traditionally strong performers on classification tasks.
Our first ensemble will use gradient boosting via the `gbm` model.

A tuning grid is used to estimate
optimal hyperparameters and generalization error is estimated with a .632 
bootstrap using `caret`.

```{r include=TRUE, message=FALSE}
set.seed(1712)
# registerDoMC(4)  # reduce cores if crashing. gbm can use a lot of memory.

f <- "fit.gbm.RDS"
if (file.exists(f)) {
    fit.gbm <- readRDS(f)
} else {
    tg <- expand.grid(n.trees=c(350, 450),
                      interaction.depth = 5:6,
                      shrinkage = c(0.03, 0.1),
                      n.minobsinnode = 10)
    tc <- trainControl(method = "boot632", number = 3, allowParallel = TRUE)
    fit.gbm <- train(classe ~ ., method="gbm", data=df.train, 
                     trControl=tc, tuneGrid=tg,
                     verbose=FALSE)
    saveRDS(object = fit.gbm, file = f)
}
colMeans(fit.gbm$resample[, 1:2])
```

These are superb results compared to our baseline, perhaps too good. We'll
investigate the possibility of overfitting in the next section.
The parameters of the best model found were as follows:

```{r include=TRUE}
fit.gbm$bestTune %>% as_tibble() %>% 
    kable() %>% 
    kable_styling(full_width=FALSE, position="left")
```

### Use an explicit train/validate set

The bootstrap results were suspiciously good and we should suspect overfitting
even though boosted models overfit slowly.
Checking our results by using the best model parameters we found, re-training
on a randomly selected training set and checking accuracy with an independent 
validation set we obtain these results:

```{r}
set.seed(42) # make sure to use a different seed
inVal <- createDataPartition(df.train$classe, p=0.3, list=FALSE)
validation <- df.train[inVal, ]
training <- df.train[-inVal, ]
```

```{r include=TRUE}
best.params <- fit.gbm$bestTune
set.seed(42)
f <- "fit.gbm.tt.RDS"
if (file.exists(f)) {
    fit.gbm.tt <- readRDS(f)
} else {
    # Use best parameters and don't create bootstrap estimates.
    tc <- trainControl(method = "none", allowParallel = TRUE)
    fit.gbm.tt <- train(classe ~ ., method="gbm", data=training, trControl=tc,
                        tuneGrid=expand.grid(best.params),
                        verbose=FALSE)
    saveRDS(object = fit.gbm.tt, file = f)
}
preds <- predict(fit.gbm.tt, validation)
postResample(preds, validation$classe)
```

```{r include=TRUE}
confusionMatrix(preds, validation$classe)$byClass |> 
    as_tibble(rownames="Prediction") |> 
    select(1,2,3,balanced.accuracy=12) |> 
    kable() |> 
    kable_styling(full_width=FALSE, position="left")
```


The test performance, 99.27%, is very close to the .632 bootstrap estimate.  We can conclude
that the model is finding a true predictive structure in the data.

Let's also examine the top six most important variables for the boosted model:

```{r include=TRUE}
gbm.importance <- varImp(fit.gbm.tt$finalModel) %>% 
    arrange(desc(Overall)) %>% 
    slice_head(n=6) %>% 
    mutate(Rank.gbm = rank(max(Overall) - Overall))
    
gbm.importance[, "Overall", drop=F] %>% kable() %>% 
    kable_styling(full_width=FALSE, position="left")
```

## Random Forest

The first step is to obtain a good estimate for `mtry`, the number of variables
to consider at each split of a tree. This is the only tuning parameter we'll consider,
and we'll use specialized method `randomForest::tuneRF` to get a good estimate.

```{r include=TRUE}
set.seed(1712)
f <- "tuned.rf.RDS"
if (file.exists(f)) {
    tuned.rf <- readRDS(f)
} else {
    tuned.rf <- tuneRF(x=subset(training, select=-classe), y=training$classe,
                    stepFactor = 1.5, improve=0.001, mtryStart = 7,
                    ntreeTry = 150, doBest=TRUE)
    saveRDS(object = tuned.rf, file = f)
}
tuned.rf
```

Results suggest the optimal value for `mtry` is around 10. 
Literature suggests a value for `mtry` around 
$\sqrt{\text{number of features}} = \sqrt{55} \approx 7$ which is close.
We'll use 10 in our model. Fitting a model with `mtry=10` on the same training 
set we used with the boosting method, we get these results:

```{r include=TRUE}
library(doMC) # parallel backend
registerDoMC(4)
set.seed(1712)
f <- "fit.rf.RDS"
if (file.exists(f)) {
    fit.rf <- readRDS(f)
} else {
    tc <- trainControl(method = "none", allowParallel = TRUE)
    tg <- expand.grid(mtry=10)
    fit.rf <- train(classe ~ ., method="rf", data=training, ntree=500,
                        trControl=tc, tuneGrid=tg)
    saveRDS(object = fit.rf, file = f)
}
preds <- predict(fit.rf, validation)
postResample(preds, validation$classe)
# confusionMatrix(preds, validation$classe)
```

```{r include=TRUE}
confusionMatrix(preds, validation$classe)$byClass |> 
    as_tibble(rownames="Prediction") |> 
    select(1,2,3,balanced.accuracy=12) |> 
    kable() |> 
    kable_styling(full_width=FALSE, position="left")
```

The random forest obtains 99.3% accuracy on the validation set, compared to
99.26% for the boosted model. Their class-specific sensitivity, specificity,
etc are also near-equal. The models are essentially equivalent in their
predictive ability, though the random forest was much faster to train.

Using the generic `importance` function to list variable importance,
the top six variables for the random forest are the same  the boosted model,
suggesting both models are finding similar structures in the data, which
increases our confidence in their correctness.

```{r include=TRUE}
importance(fit.rf$finalModel) %>% as_tibble(rownames = 'variable') %>%
    arrange(desc(MeanDecreaseGini)) %>% slice_head(n=6) %>% 
    kable() %>% 
    kable_styling(full_width=FALSE, position="left")
# varImpPlot(fit.rf$finalModel)
```

# Predictions

A test dataset of 20 observations has been provided. We'll predict using our
boosted and random forest models and compare them to each other.

```{r echo=TRUE, include=TRUE}
pred.gbm.tt <- predict(fit.gbm.tt, df.test)
pred.rf <- predict(fit.rf, df.test)
table(pred.gbm.tt == pred.rf)
```

Both models predict the same values for the test set.

# Conclusions

After reducing the number of variables from 160 to 52, we were able to build
both random forest and boosted models with an accuracy of 99.3%, equivalent
to be one false detection in 143 repetitions of a bicep curl. It
seems likely that a combination of smart devices and machine learning are
genuinely able to detect correct vs incorrect exercise form. 

To create a real-world product there would be much more work to do, starting with
creating a much larger dataset including all the other types of exercise the smart device
would support - this dataset only contains observations of one exercise.
Other complications such as expert consultation with exercise physiologists
and liability issues would have to be taken into account, so this is where
I will end this project.

# References

Original Dataset:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, [H. Qualitative Activity Recognition of Weight Lifting Exercises.](https://www.researchgate.net/publication/266653495_Qualitative_activity_recognition_of_weight_lifting_exercises) Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 

Obtained from [https://web.archive.org/web/20150207080848/http://groupware.les.inf.puc-rio.br/har](https://web.archive.org/web/20150207080848/http://groupware.les.inf.puc-rio.br/har)



