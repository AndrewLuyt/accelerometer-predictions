---
title: 'Project: Weightlifting Dataset Prediction'
author: "Andrew Luyt"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include=FALSE)
library(knitr)
library(kableExtra)
library(tidyverse)
library(lubridate)
library(ModelMetrics) # do before caret so we don't mask e.g. confusionMatrix
library(caret)
library(randomForest)
library(gbm)
library(glmnet)
# library(doMC) # parallel backend
# registerDoMC(4)
```

# Notes on dataset

There are five classes corresponding to correct & incorrect form while doing
the exercise. This is the `classe` variable.

* **A**: correct form
* **B-E**: incorrect form (common mistakes)
    * B: throwing the elbows to the front
    * C: lifting the dumbbell only halfway
    * D: lowering the dumbbell only halfway
    * E: throwing the hips to the front
    
**There are 158 variables available for prediction.**  Variable selection will
be an important part of this problem. 
    
# Intent of dataset

To see if a machine learning model can distinguish between correct and incorrect 
exercuse form based on readings from accelerometers on the body & dumbbell while performing the exercise.

If the classes could be distinguished automatically and in real-time, one 
possible application would be a wearable device
letting off a notification/alarm to tell the user their form is suspect:
e.g. throwing elbows to the front or thrusting their hips forward. 

# Benchmarks

The original researchers obtained 78.5% overall accuracy on this dataset, with class-specific accuracies between 74% and 86%.

![Figure by Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.](wle_confusion_matrix.png){width=50% height=50%}


# Cleaning and preprocessing


```{r}
# There are a few ways NAs show up in the data
df.train <- read_csv("pml-training.csv", na = c("NA", "#DIV/0!", ""))
df.test <- read_csv("pml-testing.csv", na = c("NA", "#DIV/0!", ""))
```

There are 159 variables availble. Using `skimr::skim()` we find that 100 them
have over 97% of their data missing.

```{r}
df.train %>% skimr::skim() %>% 
    filter(complete_rate < 0.03) %>% 
    pull(skim_variable) %>% 
    length()
```

We'll simply remove these variables. *This greatly simplifies the problem of variable selection* as we now only have 59 predictors available. Looking further we find
a few more problems.

```{r}
vars.remove <- df.train %>% 
    skimr::skim() %>% 
    filter(complete_rate < 0.03) %>% 
    pull(skim_variable)
```

`cvtd_timestamp` is the time the user performed the exercise *during the original
experiment* and
will have no bearing on *future* prediction.  `user_name`, `new_window` and 
`classe` need to be converted to factors.  

```{r}
vars.remove <- c(vars.remove, 'cvtd_timestamp')
```

The variable `raw_timestamp_part_1` is *suspicious.*  In the plot below
we can distinguish 
the blocks of time where each `classe` was performed by each user. This appears
to be due simply to the way the original experiment was conducted: each subject
was asked to complete a set of exercises, one after another and these times
simply reflect that. The issue is that they show a *very strong* predictive 
pattern in the training data. When an algorithm is applied on *future* data, for
instance a non-experimental subject doing their normal workout in a gym,
these time patterns may be completely different. 
We will remove this variable along with `raw_timestamp_part_2`.

```{r include=TRUE}
df.train %>% ggplot(aes(user_name, raw_timestamp_part_1, col=classe)) + geom_boxplot()
```

`num_window` has similar problems. Below, if for example `num_window` is
about 250, the `classe` will be **B**. This variable seems to be an identifier
for blocks of information sequentially extracted from the accelerometer data 
stream during feature extraction. There's no reason to suspect this pattern 
would continue in any future data.

```{r include=TRUE}
df.train %>% ggplot(aes(num_window, classe, col=classe)) + geom_point()
```


```{r}
vars.remove <- c(vars.remove, 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'num_window')
```

```{r}

clean_df <- function(df) {
    df <- df %>% 
        select(!...1) %>%  # row number, useless
        select(!all_of(vars.remove)) %>% 
        mutate(user_name = as_factor(user_name),
               new_window = as_factor(new_window))
    if ("classe" %in% names(df)) {
        df <- mutate(df, classe = as_factor(classe))
    }
    df
}

df.train <- clean_df(df.train)
df.test <- clean_df(df.test)
```

## Check variables for normality

Some models, like LDA, assume normal distributions of variables - how good of an assumption would this be?  Using a Shapiro Test, we find that no variable is
drawn from a normal distribution.

```{r}
set.seed(1712)
sample.df <- df.train %>% slice_sample(n = 3000) # shapiro needs < 5000 items
shapiro.results <- lapply(X = select(sample.df, where(is.numeric)), FUN = shapiro.test)
shapiro.results <- sapply(shapiro.results, function(i) i$p.value)
shapiro.results[shapiro.results > 1e-15]; rm(shapiro.results)
```


## Check for near-zero-variance predictors

After removing variables full of NAs, `caret::nearZeroVar()` was used. All 
variables have sufficient variance to be useful, in theory.

```{r}
df.train %>% select(where(is.numeric)) %>% 
    nearZeroVar(saveMetrics = T, allowParallel = TRUE) %>% 
    filter(nzv==TRUE)
```

# Modeling

We'll explore an Linear Discriminant model first as a baseline. We found that
the variables are not normally distributed, so we can't expect good 
classification performance.

A .632 bootstrapping estimate is used at first to get an estimate of test
error while using the dataset efficiently.

```{r}
y <- df.train$classe
```


## Linear Discriminant Model

`classe` was fit to all variables and the resulting accuracy estimate was 73%.
We note the original researchers obtained results about five percentage points
higher.

```{r include=TRUE}
set.seed(1712)
tc <- trainControl(method = "boot632", number = 10, allowParallel = TRUE)
fit.lda <- train(classe ~ ., method='lda', data=df.train, trControl=tc)
preds <- predict(fit.lda)
round(fit.lda$results[2:3], 3)
# postResample(preds, y)
# round(confusionMatrix(preds, y, )$overall, 3)
```

Next we will try two ensemble models generally considered to give excellent
performance on many classification problems: boosted trees and random forests.

## Stochastic Gradient Boosting

We'll use the `gbm` model. A tuning grid is used to estimate
optimal hyperparameters and error is estimated with a .632 bootstrap
using `caret`.

```{r include=TRUE, message=FALSE}
set.seed(1712)
# parallel processing seems to crash rstudio consistently?
library(doMC) # parallel backend
registerDoMC(2)
f <- "fit.gbm.RDS"
if (file.exists(f)) {
    fit.gbm <- readRDS(f)
} else {
    tg <- expand.grid(n.trees=c(350, 450),
                      interaction.depth = 5:6,
                      shrinkage = c(0.03, 0.1),
                      n.minobsinnode = 10)
    tc <- trainControl(method = "boot632", number = 3, allowParallel = TRUE)
    fit.gbm <- train(classe ~ ., method="gbm", data=df.train, 
                     trControl=tc, tuneGrid=tg,
                     verbose=FALSE)
    saveRDS(object = fit.gbm, file = f)
}
colMeans(fit.gbm$resample[, 1:2])
# preds <- predict(fit.gbm)
# postResample(preds, y)
```

The parameters of the best model found were as follows:

```{r include=TRUE}
fit.gbm$bestTune %>% as_tibble() %>% 
    kable() %>% 
    kable_styling(full_width=FALSE, position="left")
```

### Use an explicit train/validate set

The bootstrap results were suspiciously good and we should suspect overfitting
even though boosted models overfit slowly.
Checking our results by using the best model parameters we found, re-training
on a training set and checking accuracy with an independent validation set we
again obtain these results:

```{r}
set.seed(1712)
inVal <- createDataPartition(df.train$classe, p=0.3, list=FALSE)
validation <- df.train[inVal, ]
training <- df.train[-inVal, ]
```

```{r include=TRUE}
best.params <- fit.gbm$bestTune
set.seed(1712)
f <- "fit.gbm.tt.RDS"
if (file.exists(f)) {
    fit.gbm.tt <- readRDS(f)
} else {
    # Use best parameters and don't create bootstrap estimates.
    tc <- trainControl(method = "none", allowParallel = TRUE)
    fit.gbm.tt <- train(classe ~ ., method="gbm", data=training, trControl=tc,
                        tuneGrid=expand.grid(best.params),
                        verbose=FALSE)
    saveRDS(object = fit.gbm.tt, file = f)
}
preds <- predict(fit.gbm.tt, validation)
postResample(preds, validation$classe)
#confusionMatrix(preds, validation$classe)
```

The results seem to be real, and the test error is very close to the .632
bootstrap estimate.

Let's also examine the top six most important variables for the boosted model:

```{r include=TRUE}
gbm.importance <- varImp(fit.gbm.tt$finalModel) %>% 
    arrange(desc(Overall)) %>% 
    slice_head(n=6) %>% 
    mutate(Rank.gbm = rank(max(Overall) - Overall))
    
gbm.importance[, "Overall", drop=F] %>% kable() %>% 
    kable_styling(full_width=FALSE, position="left")
```

## Random Forest

The first step is to obtain a good estimate for `mtry`, the number of variables
to consider at each split of a tree. This is the only tuning parameter available,
and the `randomForest` package has a specialized method for getting a good
estimate, `tuneRF`, so we'll use that.  

```{r include=TRUE}
set.seed(1712)
f <- "tuned.rf.RDS"
if (file.exists(f)) {
    tuned.rf <- readRDS(f)
} else {
    tuned.rf <- tuneRF(x=subset(training, select=-classe), y=training$classe,
                    stepFactor = 1.5, improve=0.001, mtryStart = 7,
                    ntreeTry = 150, doBest=TRUE)
    saveRDS(object = tuned.rf, file = f)
}
tuned.rf
```

Results suggest the optimal value for `mtry` is around 10. 
Literature suggests a value for `mtry` around 
$\sqrt{\text{number of features}} = \sqrt{55} \approx 7$ which is close.
We'll use 10 in our model. Fitting a model with `mtry=10` on the same training 
set we used with the boosting method, we get these results:

```{r include=TRUE}
library(doMC) # parallel backend
registerDoMC(4)
set.seed(1712)
f <- "fit.rf.RDS"
if (file.exists(f)) {
    fit.rf <- readRDS(f)
} else {
    tc <- trainControl(method = "none", allowParallel = TRUE)
    tg <- expand.grid(mtry=10)
    fit.rf <- train(classe ~ ., method="rf", data=training, ntree=500,
                        trControl=tc, tuneGrid=tg)
    saveRDS(object = fit.rf, file = f)
}
preds <- predict(fit.rf, validation)
postResample(preds, validation$classe)
# confusionMatrix(preds, validation$classe)
```

The random forest obtains 99.6% accuracy on the validation set, compared to
99.5% for the boosted model. The models are nearly equivalent in their
predictive ability, though the random forest was much faster to train.

Using the generic `importance` function to list variable importance,
the top six variables are identical to the boosted model, suggesting both models 
are finding similar structures in the data.

```{r include=TRUE}
importance(fit.rf$finalModel) %>% as_tibble(rownames = 'variable') %>%
    arrange(desc(MeanDecreaseGini)) %>% slice_head(n=6) %>% 
    kable() %>% 
    kable_styling(full_width=FALSE, position="left")
# varImpPlot(fit.rf$finalModel)
```

# Predictions

A test dataset of 20 observations has been provided. We'll predict using our
boosted and random forest models and compare them to each other.

```{r echo=TRUE, include=TRUE}
pred.gbm.tt <- predict(fit.gbm.tt, df.test)
pred.rf <- predict(fit.rf, df.test)
table(pred.gbm.tt == pred.rf)
```

Both models predict the same values for the test set.

# References

Original Dataset:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 













